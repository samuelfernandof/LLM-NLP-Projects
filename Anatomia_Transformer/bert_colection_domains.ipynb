{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "560c0efc",
      "metadata": {
        "id": "560c0efc"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8a248c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f8a248c",
        "outputId": "483902da-6111-4a2b-d7c7-040f09910301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8c8e0f",
      "metadata": {
        "id": "9d8c8e0f"
      },
      "outputs": [],
      "source": [
        "# Importação das bibliotecas necessárias\n",
        "import transformers\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3242ac36",
      "metadata": {
        "id": "3242ac36",
        "outputId": "6f6021d9-bd70-4134-fe38-8d691c567b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Author: Data Science Academy\n",
            "\n",
            "torch       : 1.13.1\n",
            "transformers: 4.28.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Data Science Academy\" --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40471d7c",
      "metadata": {
        "id": "40471d7c"
      },
      "source": [
        "## Arquitetura BioBERT\n",
        "\n",
        "Usaremos o modelo BioBERT para classificar se uma sentença de texto é relacionada à saúde. Primeiro, definimos o modelo e o tokenizador BioBERT usando a biblioteca transformers. Em seguida, tokenizamos um texto de exemplo, passamos os tokens pelo modelo BioBERT para obter as saídas e, finalmente, usamos a saída da última camada para classificar a sentença.\n",
        "\n",
        "https://huggingface.co/dmis-lab/biobert-v1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6aa6749",
      "metadata": {
        "id": "b6aa6749"
      },
      "outputs": [],
      "source": [
        "# Definição do modelo BioBERT\n",
        "model = transformers.AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7879dc50",
      "metadata": {
        "id": "7879dc50"
      },
      "outputs": [],
      "source": [
        "# Definição do tokenizador BioBERT\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7424c094",
      "metadata": {
        "id": "7424c094"
      },
      "outputs": [],
      "source": [
        "# Texto de exemplo\n",
        "text = \"A insulina é um hormônio que ajuda a regular o açúcar no sangue\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4aa53f",
      "metadata": {
        "id": "1e4aa53f"
      },
      "outputs": [],
      "source": [
        "# Tokenização do texto de exemplo\n",
        "tokens = tokenizer.encode_plus(text,\n",
        "                               max_length = 128,\n",
        "                               truncation = True,\n",
        "                               padding = \"max_length\",\n",
        "                               return_tensors = \"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c77316",
      "metadata": {
        "id": "34c77316",
        "outputId": "f73ac2ab-c75e-4bbf-9fa4-110f04e357ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,   138, 26825,  1161,   255, 15276, 16358,  9019, 28206, 22772,\n",
              "         15027,   170,  9380,  1810,   170,  2366,   184,   170, 28201, 12643,\n",
              "          8766,  1185,  6407,  4175,   102,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a9a969",
      "metadata": {
        "id": "b5a9a969"
      },
      "outputs": [],
      "source": [
        "# Passando os tokens pelo modelo BioBERT para obter as saídas\n",
        "outputs = model(**tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3f97a6",
      "metadata": {
        "id": "9d3f97a6"
      },
      "outputs": [],
      "source": [
        "# Classificação da sentença de acordo com a tarefa\n",
        "# Aqui, por exemplo, estamos usando a saída da última camada para classificar a sentença\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "classification = torch.argmax(last_hidden_state[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba931d0",
      "metadata": {
        "id": "5ba931d0",
        "outputId": "f8bb352d-edfc-46da-e3eb-26323e73ed9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A classificação da sentença 'A insulina é um hormônio que ajuda a regular o açúcar no sangue' é 98\n"
          ]
        }
      ],
      "source": [
        "# Imprimindo a classificação da sentença\n",
        "print(f\"A classificação da sentença '{text}' é {classification}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6876d21",
      "metadata": {
        "id": "a6876d21"
      },
      "source": [
        "## Arquitetura SciBERT\n",
        "\n",
        "Aqui usamos o modelo SciBERT para classificar o sentimento de um artigo científico. Primeiro, definimos o modelo e o tokenizador SciBERT usando a biblioteca transformers. Em seguida, tokenizamos um texto de exemplo, passamos os tokens pelo modelo SciBERT para obter as saídas e, finalmente, usamos a saída da última camada para classificar o sentimento do artigo.\n",
        "\n",
        "https://huggingface.co/allenai/scibert_scivocab_cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61bf9585",
      "metadata": {
        "id": "61bf9585",
        "outputId": "8d24eb72-be89-4007-881d-b0cd3693a539"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Definição do modelo SciBERT\n",
        "model = transformers.AutoModel.from_pretrained(\"allenai/scibert_scivocab_cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0341982",
      "metadata": {
        "id": "f0341982"
      },
      "outputs": [],
      "source": [
        "# Definição do tokenizador SciBERT\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90fd1d0d",
      "metadata": {
        "id": "90fd1d0d"
      },
      "outputs": [],
      "source": [
        "# Texto de exemplo\n",
        "text = \"Este artigo apresenta uma revisão sistemática dos estudos recentes sobre os efeitos do COVID-19 no cérebro.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb545389",
      "metadata": {
        "id": "fb545389"
      },
      "outputs": [],
      "source": [
        "# Tokenização do texto de exemplo\n",
        "tokens = tokenizer.encode_plus(text,\n",
        "                               max_length = 128,\n",
        "                               truncation = True,\n",
        "                               padding = \"max_length\",\n",
        "                               return_tensors = \"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31be4bca",
      "metadata": {
        "id": "31be4bca"
      },
      "outputs": [],
      "source": [
        "# Passando os tokens pelo modelo SciBERT para obter as saídas\n",
        "outputs = model(**tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c22cb25",
      "metadata": {
        "id": "5c22cb25"
      },
      "outputs": [],
      "source": [
        "# Classificação do sentimento do artigo científico\n",
        "# Aqui, por exemplo, estamos usando a saída da última camada para classificar o sentimento\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "classification = torch.argmax(last_hidden_state[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b920752",
      "metadata": {
        "id": "3b920752",
        "outputId": "d07cd391-8972-47be-cd63-2e3be123ff3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O sentimento do artigo 'Este artigo apresenta uma revisão sistemática dos estudos recentes sobre os efeitos do COVID-19 no cérebro.' é positivo\n"
          ]
        }
      ],
      "source": [
        "# Imprimindo a classificação do sentimento\n",
        "if classification == 0:\n",
        "    print(f\"O sentimento do artigo '{text}' é negativo\")\n",
        "else:\n",
        "    print(f\"O sentimento do artigo '{text}' é positivo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c7e557",
      "metadata": {
        "id": "e7c7e557"
      },
      "source": [
        "## Arquitetura FinBERT\n",
        "\n",
        "Aqui usamos o modelo FinBERT para classificar o sentimento de uma notícia financeira. Primeiro, definimos o modelo e o tokenizador FinBERT usando a biblioteca transformers. Em seguida, tokenizamos um texto de exemplo, passamos os tokens pelo modelo FinBERT para obter as saídas e, finalmente, usamos a saída da última camada para classificar o sentimento da notícia financeira.\n",
        "\n",
        "https://huggingface.co/ProsusAI/finbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49cdcbec",
      "metadata": {
        "id": "49cdcbec",
        "outputId": "702a89cf-f2b9-4110-813c-5b96dead086d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ProsusAI/finbert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Definição do modelo FinBERT\n",
        "model = transformers.AutoModel.from_pretrained(\"ProsusAI/finbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e816b0",
      "metadata": {
        "id": "e4e816b0"
      },
      "outputs": [],
      "source": [
        "# Definição do tokenizador FinBERT\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85eb37a",
      "metadata": {
        "id": "d85eb37a"
      },
      "outputs": [],
      "source": [
        "# Texto de exemplo\n",
        "text = \"As ações da empresa X subiram após o anúncio dos lucros trimestrais.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b3a73c5",
      "metadata": {
        "id": "4b3a73c5"
      },
      "outputs": [],
      "source": [
        "# Tokenização do texto de exemplo\n",
        "tokens = tokenizer.encode_plus(text, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572e5ffd",
      "metadata": {
        "id": "572e5ffd"
      },
      "outputs": [],
      "source": [
        "# Passando os tokens pelo modelo FinBERT para obter as saídas\n",
        "outputs = model(**tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1af435",
      "metadata": {
        "id": "2b1af435"
      },
      "outputs": [],
      "source": [
        "# Classificação do sentimento da notícia financeira\n",
        "# Aqui, por exemplo, estamos usando a saída da última camada para classificar o sentimento\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "classification = torch.argmax(last_hidden_state[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a29e86c",
      "metadata": {
        "id": "0a29e86c",
        "outputId": "5e27cb7f-e12b-46cb-b071-e6720cb9e30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O sentimento da notícia financeira 'As ações da empresa X subiram após o anúncio dos lucros trimestrais.' é positivo\n"
          ]
        }
      ],
      "source": [
        "# Imprimindo a classificação do sentimento\n",
        "if classification == 0:\n",
        "    print(f\"O sentimento da notícia financeira '{text}' é negativo\")\n",
        "else:\n",
        "    print(f\"O sentimento da notícia financeira '{text}' é positivo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744ea412",
      "metadata": {
        "id": "744ea412"
      },
      "source": [
        "## Arquitetura ByT5 (Bidirectional Encoder-Decoder Transformer-5)\n",
        "\n",
        "A arquitetura ByT5 (Bidirectional Encoder-Decoder Transformer-5) é um modelo de processamento de linguagem natural proposto pelo Google Brain em 2021. É uma extensão da arquitetura T5 (Transformer-5) que utiliza um codificador e um decodificador com conexões de atenção para gerar saídas de texto de alta qualidade para uma variedade de tarefas de PLN. O modelo é pré-treinado em um grande conjunto de dados de texto em vários idiomas e tarefas, e pode ser finamente ajustado para uma ampla gama de tarefas de PLN, incluindo tradução automática, sumarização de texto, perguntas e respostas, entre outros.\n",
        "\n",
        "A arquitetura ByT5 é baseada no modelo Transformer, que usa conexões de atenção para permitir que o modelo capture informações de contexto de palavras e frases em torno da palavra atual. Ele também incorpora a ideia de codificação e decodificação bidirecionais, permitindo que o modelo leve em consideração o contexto tanto antes quanto depois da palavra atual. O modelo ByT5 é composto por um codificador bidirecional e um decodificador unidirecional, que trabalham juntos para gerar saídas de texto de alta qualidade.\n",
        "\n",
        "O modelo ByT5 é treinado usando uma técnica conhecida como tarefa de preenchimento de lacunas, na qual uma palavra ou frase é removida do texto e o modelo é treinado para preencher essa lacuna corretamente. O modelo é treinado em várias tarefas de preenchimento de lacunas em vários idiomas e em várias tarefas de PLN, o que permite que o modelo capture uma ampla gama de informações de contexto em diferentes domínios.\n",
        "\n",
        "Este exemplo ilustra o uso da arquitetura ByT5 para traduzir uma frase em inglês para francês. Primeiro, definimos o modelo e o tokenizador ByT5 usando a biblioteca transformers. Em seguida, tokenizamos o texto de exemplo em inglês e usamos o modelo ByT5 para gerar a tradução em francês. Finalmente, decodificamos a saída do modelo e imprimimos a tradução."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33601da",
      "metadata": {
        "id": "b33601da"
      },
      "outputs": [],
      "source": [
        "# Definição do modelo ByT5\n",
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e628821",
      "metadata": {
        "id": "3e628821"
      },
      "outputs": [],
      "source": [
        "# Definição do tokenizador ByT5\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c42ef9",
      "metadata": {
        "id": "c1c42ef9"
      },
      "outputs": [],
      "source": [
        "# Texto de exemplo longo\n",
        "text = \"This is a long text about the benefits of exercise. Exercise is important for \\\n",
        "maintaining physical and mental health. It can help to reduce the risk of chronic diseases such as diabetes, \\\n",
        "heart disease, and cancer. Exercise can also improve mood, reduce stress and anxiety, and improve sleep quality. \\\n",
        "There are many different types of exercise, including aerobic exercise, strength training, and flexibility \\\n",
        "exercises. It is important to find an exercise routine that works for you and that you enjoy doing.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de00d99",
      "metadata": {
        "id": "9de00d99"
      },
      "outputs": [],
      "source": [
        "# Tokenização do texto de exemplo\n",
        "tokens = tokenizer.prepare_seq2seq_batch(src_texts = [text],\n",
        "                                         max_length = 512,\n",
        "                                         return_tensors = \"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63d22c6",
      "metadata": {
        "id": "f63d22c6"
      },
      "outputs": [],
      "source": [
        "# Resumo do texto usando o modelo ByT5\n",
        "summary = model.generate(**tokens,\n",
        "                         num_beams = 4,\n",
        "                         length_penalty = 2.0,\n",
        "                         max_length = 150,\n",
        "                         early_stopping = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c75cea",
      "metadata": {
        "id": "31c75cea"
      },
      "outputs": [],
      "source": [
        "# Decodificação da saída do modelo\n",
        "summarized_text = tokenizer.decode(summary[0], skip_special_tokens = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac6af597",
      "metadata": {
        "id": "ac6af597",
        "outputId": "f4a91b32-98fe-403d-ca57-7cb03cafcf02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O resumo do texto é: exercise can help to reduce the risk of chronic diseases such as diabetes, heart disease, and cancer. Exercise can also improve mood, reduce stress and anxiety, and improve sleep quality.\n"
          ]
        }
      ],
      "source": [
        "# Imprimindo o resumo\n",
        "print(f\"O resumo do texto é: {summarized_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e66556",
      "metadata": {
        "id": "a9e66556"
      },
      "source": [
        "# Fim"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}