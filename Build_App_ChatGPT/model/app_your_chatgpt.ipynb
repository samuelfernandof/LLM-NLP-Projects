{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "28ba643f",
      "metadata": {
        "id": "28ba643f"
      },
      "source": [
        "\n",
        "\n",
        "## <font color='blue'>Lab 2</font>\n",
        "## <font color='blue'>Construindo Seu Próprio ChatGPT Para Geração de Texto com PyTorch</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "560c0efc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560c0efc",
        "outputId": "ad7aa801-f99a-4bca-e4f1-e6cf6b390f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8a248c",
      "metadata": {
        "id": "1f8a248c"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34233957",
      "metadata": {
        "id": "34233957",
        "outputId": "042117fa-036b-41b0-8981-82d0f236b9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TF_CPP_MIN_LOG_LEVEL=3\n"
          ]
        }
      ],
      "source": [
        "%env TF_CPP_MIN_LOG_LEVEL=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8c8e0f",
      "metadata": {
        "id": "9d8c8e0f"
      },
      "outputs": [],
      "source": [
        "# Importação das bibliotecas necessárias\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3242ac36",
      "metadata": {
        "id": "3242ac36",
        "outputId": "af9bd999-9218-47dd-b232-d2e5cfb5b2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Author: Data Science Academy\n",
            "\n",
            "transformers: 4.28.1\n",
            "torch       : 2.0.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Data Science Academy\" --iversions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e15450a6",
      "metadata": {
        "id": "e15450a6"
      },
      "outputs": [],
      "source": [
        "# Função para geração de texto\n",
        "def gera_texto(prompt, model, tokenizer, device, max_length = 50, num_return_sequences = 1):\n",
        "\n",
        "    # Coloca o modelo em modo de previsão\n",
        "    model.eval()\n",
        "\n",
        "    # Extrai os tokens do texto de entrada no formato do PyTorch (pt)\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
        "\n",
        "    # Gera sequências de texto a partir do texto inicial\n",
        "    generated_sequences = model.generate(input_ids = input_ids,\n",
        "                                         max_length = max_length,\n",
        "                                         num_return_sequences = num_return_sequences,\n",
        "                                         no_repeat_ngram_size = 2,\n",
        "                                         pad_token_id = tokenizer.eos_token_id,\n",
        "                                         do_sample = True,\n",
        "                                         top_k = 50,\n",
        "                                         top_p = 0.95,\n",
        "                                         temperature = 0.8)\n",
        "\n",
        "    return [tokenizer.decode(sequence, skip_special_tokens = True) for sequence in generated_sequences]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be8cf2a",
      "metadata": {
        "id": "8be8cf2a"
      },
      "source": [
        "No código acima, model.generate() é uma função usada para gerar sequências de texto a partir de um modelo de linguagem. Cada argumento que a função recebe tem um propósito específico:\n",
        "\n",
        "**input_ids**: Este é o identificador de entrada para o modelo. Geralmente é uma sequência de números que representam o texto de entrada, transformado em tokens por um tokenizer.\n",
        "\n",
        "**max_length**: Este é o comprimento máximo da sequência de saída. O modelo irá parar de gerar quando atingir este limite.\n",
        "\n",
        "**num_return_sequences**: Este é o número de sequências independentes a serem retornadas. Cada uma será gerada de um único prompt.\n",
        "\n",
        "**no_repeat_ngram_size**: Esse é o tamanho do n-grama que não deve ser repetido. Se for 2, por exemplo, o modelo não gerará a mesma sequência de 2 tokens seguidos.\n",
        "\n",
        "**pad_token_id**: Este é o ID do token que será usado para preenchimento (padding) se a sequência gerada for mais curta que o max_length.\n",
        "\n",
        "**do_sample**: Se definido como True, a função irá usar amostragem aleatória para gerar o texto, o que significa que poderá haver variações a cada vez que a mesma entrada for fornecida. Se False, a função sempre gerará a mesma saída para uma determinada entrada.\n",
        "\n",
        "**top_k**: Este parâmetro é usado para limitar a amostragem às K principais probabilidades. O modelo seleciona um dos K tokens mais prováveis para continuar a sequência a cada passo.\n",
        "\n",
        "**top_p**: Este é o valor de \"nucleus sampling\" ou \"top-p sampling\". Ao invés de selecionar as top K probabilidades, o modelo seleciona um conjunto mínimo de tokens que tem uma probabilidade cumulativa de pelo menos p. Isso pode levar a um conjunto de tokens potencialmente menor ou maior do que K, dependendo da distribuição de probabilidade.\n",
        "\n",
        "**temperature**: Este parâmetro afeta a \"aleatoriedade\" das escolhas do modelo. Valores mais altos fazem a distribuição de probabilidade mais plana (mais aleatória), enquanto valores mais baixos tornam a distribuição mais aguda (menos aleatória). Por exemplo, uma temperatura de 0.1 fará o modelo quase sempre escolher o token mais provável, enquanto uma temperatura de 1.0 fará a seleção ser mais proporcional às probabilidades dos tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03c72a1",
      "metadata": {
        "id": "c03c72a1"
      },
      "source": [
        "https://huggingface.co/gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459ba415",
      "metadata": {
        "id": "459ba415"
      },
      "outputs": [],
      "source": [
        "# Modelo\n",
        "PRETRAINED_MODEL = 'gpt2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3165c1d2",
      "metadata": {
        "id": "3165c1d2"
      },
      "outputs": [],
      "source": [
        "# Dispositivo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00fb7a0",
      "metadata": {
        "id": "f00fb7a0"
      },
      "outputs": [],
      "source": [
        "# Inicialização do tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(PRETRAINED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e1cb4c",
      "metadata": {
        "id": "b0e1cb4c"
      },
      "outputs": [],
      "source": [
        "# Inicialização do modelo\n",
        "modelo = GPT2LMHeadModel.from_pretrained(PRETRAINED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e35819",
      "metadata": {
        "id": "02e35819",
        "outputId": "c79a8581-7969-481f-c7fe-45c88754a315"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Coloca o modelo no device\n",
        "modelo.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a056922d",
      "metadata": {
        "id": "a056922d"
      },
      "outputs": [],
      "source": [
        "# Texto inicial\n",
        "prompt = 'The advantages of artificial intelligence are'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc488855",
      "metadata": {
        "id": "dc488855"
      },
      "outputs": [],
      "source": [
        "# Gerando texto\n",
        "generated_texts = gera_texto(prompt,\n",
        "                             modelo,\n",
        "                             tokenizer,\n",
        "                             DEVICE,\n",
        "                             max_length = 100,\n",
        "                             num_return_sequences = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362321f0",
      "metadata": {
        "id": "362321f0",
        "outputId": "9f4c7d22-c039-4aa1-ddc7-fa009fe75286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto Gerado 1:\n",
            "The advantages of artificial intelligence are myriad, and Amendola is likely to be the first to admit that the future is bright.\n",
            "\n",
            "Texto Gerado 2:\n",
            "The advantages of artificial intelligence are that it is not constrained by a single set of laws and that there is no need to think in isolation. It can be applied in many different ways, and some of the most useful of them could be implemented in the real world. In fact, the human mind is so much more complex than that of a computer. There are many computational tools that can do a lot of very important things, from designing algorithms to developing artificial neural networks, but we are not fully familiar\n",
            "\n",
            "Texto Gerado 3:\n",
            "The advantages of artificial intelligence are that it is already able to understand human behavior and use information on it to do its job, whereas most humans are limited to doing their jobs in front of computers. However, human intelligence does need to have some kind of capability for human interaction, something that is much more difficult to develop and improve for the next decade.\n",
            "\n",
            "The biggest problem is that humans seem to be better at the tasks that we are doing than our computers are at solving them. The problems for\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Imprime o resultado\n",
        "for idx, generated_text in enumerate(generated_texts):\n",
        "    print(f'Texto Gerado {idx + 1}:\\n{generated_text}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e66556",
      "metadata": {
        "id": "a9e66556"
      },
      "source": [
        "# Fim"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}